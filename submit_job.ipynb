{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submitting a job with the [`otello`](https://github.com/hysds/otello) python library\n",
    "\n",
    "#### Once your job-type has been registered and built (see: [pge_create.ipynb](pge_create.ipynb)), jobs can be submitted from python using the steps laid out in this notebook.\n",
    "\n",
    "#### While this notebook only shows submission of a single job/parameter-set, you can map or iterate over a collection of input parameter sets to efficiently submit large batches of jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish an otello `Mozart` instance to communicate with the HySDS cluster controller.\n",
    "#### It will be necessary to provide credentials the first time you initialise otello.\n",
    "\n",
    "#### When prompted for the HySDS host, include the protocol, e.g. https://my-mozart.jpl.nasa.gov\n",
    "\n",
    "#### When prompted for \"HySDS cluster authenticated\", enter 'y' if the cluster requires a password to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import otello\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "if not os.path.exists(f\"{Path.home()}/.config/otello/config.yml\"):\n",
    "    otello.client.initialize()\n",
    "\n",
    "m = otello.mozart.Mozart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a `JobType` object\n",
    "\n",
    "#### *You will need to ensure that the the tag (i.e. the portion following the colon) matches the branch name used by your repository.*\n",
    "\n",
    "#### *You will also need to replace `<YOUR_JOB_TYPE_NAME>` with the name of your job's notebook (without the filetype extension)*\n",
    "#### e.g. if your notebook is hello_world_sample_pge.ipynb, the hysds-io.json and job-spec.jsons will have suffix `hello_world_sample_pge` and the value of `job_type` below would be `job-hello_world_sample_pge:develop` or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type_name = 'job-<YOUR_JOB_TYPE_NAME>:develop'  # This will need to be customised using the relevant action name (ie job/hysds-spec suffix) and branch name.\n",
    "\n",
    "job_type = m.get_job_types()[job_type_name]\n",
    "job_type.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting useful information about the job-type\n",
    "\n",
    "These commands list (respectively)\n",
    "- the available queues\n",
    "- the input schema (parameters) of the job-type\n",
    "- the default arguments for the job-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type.get_queues()\n",
    "print(job_type.describe())\n",
    "pprint(job_type.get_input_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying arguments to pass when running the job\n",
    "\n",
    "#### Here is where user-defined parameter values are specified, making sure to remain consistent with value types as indicated in the *helloworld_notebook*. Default values are used where none are provided (as we've done here with *start_orbit_number*). *set_input_params* is called to pass the parameter values to the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_parameters = {\n",
    "    'str_arg': 'THIS IS SINGLE JOB 1'\n",
    "}\n",
    "\n",
    "job_type.set_input_params(custom_parameters)\n",
    "pprint(job_type.get_input_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting the job\n",
    "#### A job tag (useful for finding the job later) and job queue are specified. Both are optional. Job submission is asynchronous, so this call will return almost immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "sample_job_tag = f'{datetime.strftime(datetime.now(), \"%Y%m%d\")}_single_submission_job_test'\n",
    "print(sample_job_tag)\n",
    "\n",
    "job_run = job_type.submit_job(tag=sample_job_tag, queue=\"factotum-job_worker-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining job completion\n",
    "#### Information about the job state will print periodically, until the job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "job_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Multiple Jobs: Specifying input argument sets\n",
    "#### It is possible to iterate through a list of input argument sets and submit many jobs asynchronously.  Jobs will be processed in parallel using the PCM cluster's compute node fleet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_tag = f'{datetime.now().strftime(\"%Y%m%d\")}multiple_submission_job_test'\n",
    "input_parameter_sets = [{\n",
    "    'tags': f'{common_tag}_1',\n",
    "    'params': {\n",
    "        'str_arg': 'THIS IS MULTIPLE JOB 1'\n",
    "    }\n",
    "},\n",
    "    {\n",
    "        'tags': f'{common_tag}_2',\n",
    "        'params': {\n",
    "            'str_arg': 'THIS IS MULTIPLE JOB 2'\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Multiple Jobs: Submitting the Jobs \n",
    "Now submit the jobs and keep jobs in otello's job_set data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_set = otello.JobSet()\n",
    "\n",
    "for parameter_set in input_parameter_sets:\n",
    "    job_type.set_input_params(parameter_set[\"params\"])\n",
    "    job = job_type.submit_job(tag=parameter_set[\"tags\"], queue=\"factotum-job_worker-small\")\n",
    "    job_set.append(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining completion of the job-set\n",
    "#### Information about the jobs' states will print periodically, until all the jobs are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_set.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting metadata for a job-set\n",
    "\n",
    "#### This metadata includes AWS S3 bucket paths where the ingested data is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "products_metadata = []\n",
    "for job in job_set:\n",
    "    try:\n",
    "        product_metadata = job.get_generated_products()\n",
    "        print(json.dumps(products_metadata, indent=2, sort_keys=True))\n",
    "        products_metadata.append(product_metadata)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authenticate to AWS\n",
    "\n",
    "#### In a terminal window, run `aws-login -p default` and enter your credentials"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the jobs' data products to a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_directories = []\n",
    "for product_metadata in products_metadata:\n",
    "    try:\n",
    "        s3_product_url = re.sub(r'^s3://.+?/(.+)$', r's3://\\1', product_metadata[0]['urls'][-1])\n",
    "\n",
    "        destination_directory = os.path.basename(s3_product_url)\n",
    "\n",
    "        if os.path.isdir(destination_directory):\n",
    "            shutil.rmtree(destination_directory)\n",
    "\n",
    "        print(f'Running \"aws s3 sync {s3_product_url} {destination_directory}\"')\n",
    "        !aws s3 sync $s3_product_url $destination_directory\n",
    "\n",
    "        product_directories.append(destination_directory)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "for product_directory in product_directories:\n",
    "    !ls $product_directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}