{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining your job inputs\n",
    "\n",
    "The following cell is tagged with \"parameters\", which allows papermill to identify the cell containing per-run parameters\n",
    "Cell tags may be accessed using the double-gear icon in JupyterLab's left-hand gutter.\n",
    "\n",
    "All variables defined in the following cell are treated as job input parameters, and will be accessible through the `_context.json` file at runtime.\n",
    "\n",
    "For more information, visit https://papermill.readthedocs.io/en/latest/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Job input parameters\n",
    "str_arg: str = \"this is a string parameter\"\n",
    "py3_list_arg: List[str] = [\"this\", \"is\", \"a\", \"list\", \"parameter\"]\n",
    "py2_list_arg = [\"python2\", \"hinting\", \"is\", \"also\", \"supported\"]  # type: List\n",
    "int_arg: int = 100\n",
    "float_arg: float = 10.2553\n",
    "enumerated_arg: \"enum\" = [\"yes\", \"no\", \"maybe so\"]\n",
    "dict_arg: Dict = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
    "inferred_str_arg = \"it is also possible for papermill to infer parameter type\"\n",
    "\n",
    "# PCM-System Parameters\n",
    "# These use reserved-prefix parameter names (_*) and are also parsed during `notebook-pge-wrapper specs` to generate the hysds-io and job-spec\n",
    "_time_limit = 57389\n",
    "_soft_time_limit = 4738\n",
    "_disk_usage = \"10GB\"\n",
    "_submission_type = \"individual\"\n",
    "_required_queue = \"factotum-job_worker-small\"\n",
    "_label = \"PGE_NAME_PLACEHOLDER\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining your process\n",
    "\n",
    "The following cell contains trivial stubbed function examples as might be used in a job execution flow.\n",
    "\n",
    "Generally, a job consists of retrieving some data based on the job's arguments, processing it somehow, and writing the output to one or more files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_data() -> str:\n",
    "    return 'This is generally an existing product or stack from either the HySDS catalog (via pele) or a 3rd-party DAAC like ASF.'\n",
    "\n",
    "def process_and_store_data(data) -> None:\n",
    "    processed_data = data.upper()\n",
    "\n",
    "    with open('my_sample_product.txt', 'w+') as out_file:\n",
    "        out_file.write(processed_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining your job outputs and metadata files\n",
    "\n",
    "The following cell contains the functions necessary to create a trivial data product for ingestion into the PCM data product catalog.\n",
    "\n",
    "These functions should be augmented to include your desired dataset definition data, metadata and job output files\n",
    "\n",
    "It is also typical to include important fields (e.g. track number, orbit direction and temporal bound timestamps) in the dataset id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath(os.curdir)\n",
    "\n",
    "def generate_dummy_context_file() -> None:\n",
    "    \"\"\"When run in HySDS, a _context.json file will be present in the working directory, so this is only necessary for local development\"\"\"\n",
    "    filepath: str = os.path.join(working_dir, '_context.json')\n",
    "    print(f'Writing dummy context to {filepath}')\n",
    "    with open(filepath, 'w+') as context_file:\n",
    "        json.dump({'run_timestamp': datetime.now().isoformat()}, context_file)\n",
    "\n",
    "def generate_dataset_id(id_prefix: str, context: str) -> str:\n",
    "    \"\"\"Generates a globally-unique ID for the data product produced.\n",
    "    Uniqueness is generally ensured by the context, which will (theoretically) be either unique, or subject to deduplication by HySDS\"\"\"\n",
    "    \n",
    "    hash_suffix = md5(context.encode()).hexdigest()[0:5]\n",
    "\n",
    "    job_id = f'{id_prefix}-{datetime.now().isoformat()}-{hash_suffix}'\n",
    "\n",
    "    print(f'Generated job ID: {job_id}')\n",
    "    return job_id\n",
    "\n",
    "\n",
    "def generate_dataset_file(dataset_id: str, **kwargs) -> None:\n",
    "    \"\"\"Stores standardized metadata used for indexing products in HySDS GRQ\"\"\"\n",
    "    dataset_definition_filepath: str = os.path.join(working_dir, dataset_id, f'{dataset_id}.dataset.json')\n",
    "    metadata: dict = {\n",
    "        'version': kwargs.get('version', 'v1.0'),\n",
    "    }\n",
    "    \n",
    "    optional_fields = [\n",
    "        'label',\n",
    "        'location',  # Must adhere to geoJSON \"geometry\" format\n",
    "        'starttime',\n",
    "        'endtime'\n",
    "    ]\n",
    "    for field in optional_fields:\n",
    "        if field in kwargs:\n",
    "            metadata[field] = kwargs.get(field)\n",
    "    \n",
    "    with open(dataset_definition_filepath, 'w+') as dataset_file:\n",
    "        print(f'Writing to {dataset_definition_filepath}')\n",
    "        json.dump(metadata, dataset_file)\n",
    "    \n",
    "def generate_metadata_file(dataset_id: str, metadata: Dict) -> None:\n",
    "    \"\"\"Stores custom metadata keys/values used for indexing products in HySDS GRQ\"\"\"\n",
    "    metadata_filepath: str = os.path.join(working_dir, dataset_id, f'{dataset_id}.met.json')\n",
    "    with open(metadata_filepath, 'w+') as metadata_file:\n",
    "        print(f'Writing to {metadata_filepath}')\n",
    "        json.dump(metadata, metadata_file)\n",
    "        \n",
    "\n",
    "        \n",
    "def generate_data_product(working_dir: str = working_dir, id_prefix: str = 'ON_DEMAND-MY_JOB_TYPE') -> None:\n",
    "    \"\"\"Generates metadata/dataset files and packages them in a specially-named directory with the desired job output files, for ingestion into the data product catalog\"\"\"\n",
    "    context_filepath: str = os.path.join(working_dir, '_context.json') \n",
    "    with open(context_filepath) as context_file:\n",
    "        context: str = context_file.read()\n",
    "            \n",
    "    dataset_id: str = generate_dataset_id(id_prefix, context)\n",
    "    \n",
    "    data_product_dir = os.path.join(working_dir, dataset_id)\n",
    "    print(f'Generating data product at {data_product_dir}')\n",
    "    \n",
    "    os.mkdir(data_product_dir)\n",
    "    generate_metadata_file(dataset_id, {'my_metadata_field': 'metadata_value'})\n",
    "    generate_dataset_file(dataset_id)\n",
    "    \n",
    "    print(f'Moving PGE output...')\n",
    "    shutil.move(os.path.join(working_dir, 'my_sample_product.txt'), os.path.join(data_product_dir, 'my_sample_product.txt'))\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining your job's high-level execution flow\n",
    "\n",
    "The following cell contains a trivial set of procedural calls, which will be run"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_dummy_context_file()\n",
    "\n",
    "data = retrieve_data()\n",
    "process_and_store_data(data)\n",
    "\n",
    "generate_data_product()\n",
    "\n",
    "print('PGE execution complete!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}